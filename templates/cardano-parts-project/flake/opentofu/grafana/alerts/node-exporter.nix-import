{
  # Newlines and spacing in expressions were added by Grafana integrations for layout purposes in the UI
  namespace = "cardano-parts-integrations";
  name = "node-exporter";
  rule = [
    {
      alert = "NodeNetworkReceiveErrs";
      annotations = {
        description = "{{ $labels.instance }} interface {{ $labels.device }} has encountered {{ printf \"%.0f\" $value }} receive errors in the last two minutes.";
        summary = "Network interface is reporting many receive errors.";
      };
      expr = "rate(node_network_receive_errs_total{job=\"integrations/node_exporter\"}[2m]) / rate(node_network_receive_packets_total{job=\"integrations/node_exporter\"}[2m]) > 0.01\n";
      for = "15m";
      labels = {severity = "warning";};
    }
    {
      alert = "NodeNetworkTransmitErrs";
      annotations = {
        description = "{{ $labels.instance }} interface {{ $labels.device }} has encountered {{ printf \"%.0f\" $value }} transmit errors in the last two minutes.";
        summary = "Network interface is reporting many transmit errors.";
      };
      expr = "rate(node_network_transmit_errs_total{job=\"integrations/node_exporter\"}[2m]) / rate(node_network_transmit_packets_total{job=\"integrations/node_exporter\"}[2m]) > 0.01\n";
      for = "15m";
      labels = {severity = "warning";};
    }
    {
      alert = "NodeHighNumberConntrackEntriesUsed";
      annotations = {
        description = "{{ $value | humanizePercentage }} of conntrack entries are used.";
        summary = "Number of conntrack are getting close to the limit.";
      };
      expr = "(node_nf_conntrack_entries{job=\"integrations/node_exporter\"} / node_nf_conntrack_entries_limit) > 0.75\n";
      labels = {severity = "warning";};
    }
    {
      alert = "NodeTextFileCollectorScrapeError";
      annotations = {
        description = "Node Exporter text file collector on {{ $labels.instance }} failed to scrape.";
        summary = "Node Exporter text file collector failed to scrape.";
      };
      expr = "node_textfile_scrape_error{job=\"integrations/node_exporter\"} == 1\n";
      labels = {severity = "warning";};
    }
    {
      alert = "NodeClockSkewDetected";
      annotations = {
        description = "Clock at {{ $labels.instance }} is out of sync by more than 300s. Ensure NTP is configured correctly on this host.";
        summary = "Clock skew detected.";
      };
      expr = "(\n  node_timex_offset_seconds{job=\"integrations/node_exporter\"} > 0.05\nand\n  deriv(node_timex_offset_seconds{job=\"integrations/node_exporter\"}[5m]) >= 0\n)\nor\n(\n  node_timex_offset_seconds{job=\"integrations/node_exporter\"} < -0.05\nand\n  deriv(node_timex_offset_seconds{job=\"integrations/node_exporter\"}[5m]) <= 0\n)            \n";
      for = "10m";
      labels = {severity = "warning";};
    }
    {
      alert = "NodeClockNotSynchronising";
      annotations = {
        description = "Clock at {{ $labels.instance }} is not synchronising. Ensure NTP is configured on this host.";
        summary = "Clock not synchronising.";
      };
      expr = "min_over_time(node_timex_sync_status{job=\"integrations/node_exporter\"}[5m]) == 0\nand\nnode_timex_maxerror_seconds{job=\"integrations/node_exporter\"} >= 16            \n";
      for = "10m";
      labels = {severity = "warning";};
    }
    {
      alert = "NodeRAIDDegraded";
      annotations = {
        description = "RAID array '{{ $labels.device }}' at {{ $labels.instance }} is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.";
        summary = "RAID Array is degraded.";
      };
      expr = "node_md_disks_required{job=\"integrations/node_exporter\",device!=\"\"} - ignoring (state) (node_md_disks{state=\"active\",job=\"integrations/node_exporter\",device!=\"\"}) > 0\n";
      for = "15m";
      labels = {severity = "critical";};
    }
    {
      alert = "NodeRAIDDiskFailure";
      annotations = {
        description = "At least one device in RAID array at {{ $labels.instance }} failed. Array '{{ $labels.device }}' needs attention and possibly a disk swap.";
        summary = "Failed device in RAID array.";
      };
      expr = "node_md_disks{state=\"failed\",job=\"integrations/node_exporter\",device!=\"\"} > 0\n";
      labels = {severity = "warning";};
    }
    {
      alert = "NodeFileDescriptorLimit";
      annotations = {
        description = "File descriptors limit at {{ $labels.instance }} is currently at {{ printf \"%.2f\" $value }}%.";
        summary = "Kernel is predicted to exhaust file descriptors limit soon.";
      };
      expr = "(\n  node_filefd_allocated{job=\"integrations/node_exporter\"} * 100 / node_filefd_maximum{job=\"integrations/node_exporter\"} > 70\n)            \n";
      for = "15m";
      labels = {severity = "warning";};
    }
    {
      alert = "NodeFileDescriptorLimit";
      annotations = {
        description = "File descriptors limit at {{ $labels.instance }} is currently at {{ printf \"%.2f\" $value }}%.";
        summary = "Kernel is predicted to exhaust file descriptors limit soon.";
      };
      expr = "(\n  node_filefd_allocated{job=\"integrations/node_exporter\"} * 100 / node_filefd_maximum{job=\"integrations/node_exporter\"} > 90\n)            \n";
      for = "15m";
      labels = {severity = "critical";};
    }
    {
      alert = "NodeCPUHighUsage";
      annotations = {
        description = "CPU usage at {{ $labels.instance }} has been above 90% for the last 15 minutes, is currently at {{ printf \"%.2f\" $value }}%.\n";
        summary = "High CPU usage.";
      };
      expr = "sum without(mode) (avg without (cpu) (rate(node_cpu_seconds_total{job=\"integrations/node_exporter\", mode!=\"idle\"}[2m]))) * 100 > 90\n";
      for = "15m";
      labels = {severity = "info";};
    }
    {
      alert = "NodeSystemSaturation";
      annotations = {
        description = "System load per core at {{ $labels.instance }} has been above 2 for the last 15 minutes, is currently at {{ printf \"%.2f\" $value }}.\nThis might indicate this instance resources saturation and can cause it becoming unresponsive.                \n";
        summary = "System saturated, load per core is very high.";
      };
      expr = "node_load1{job=\"integrations/node_exporter\"}\n/ count without (cpu, mode) (node_cpu_seconds_total{job=\"integrations/node_exporter\", mode=\"idle\"}) > 2            \n";
      for = "15m";
      labels = {severity = "critical";};
    }
    {
      alert = "NodeMemoryMajorPagesFaults";
      annotations = {
        description = "Memory major pages are occurring at very high rate at {{ $labels.instance }}, 500 major page faults per second for the last 15 minutes, is currently at {{ printf \"%.2f\" $value }}.\nPlease check that there is enough memory available at this instance.                \n";
        summary = "Memory major page faults are occurring at very high rate.";
      };
      expr = "rate(node_vmstat_pgmajfault{job=\"integrations/node_exporter\"}[5m]) > 500\n";
      for = "15m";
      labels = {severity = "warning";};
    }
    {
      alert = "NodeMemoryHighUtilization";
      annotations = {
        description = "Memory is filling up at {{ $labels.instance }}, has been above 90% for the last 15 minutes, is currently at {{ printf \"%.2f\" $value }}%.\n";
        summary = "Host is running out of memory.";
      };
      expr = "100 - (node_memory_MemAvailable_bytes{job=\"integrations/node_exporter\"} / node_memory_MemTotal_bytes{job=\"integrations/node_exporter\"} * 100) > 90\n";
      for = "15m";
      labels = {severity = "warning";};
    }
    {
      alert = "NodeDiskIOSaturation";
      annotations = {
        description = "Disk IO queue (aqu-sq) is high on {{ $labels.device }} at {{ $labels.instance }}, has been above 10 for the last 15 minutes, is currently at {{ printf \"%.2f\" $value }}.\nThis symptom might indicate disk saturation.                \n";
        summary = "Disk IO queue is high.";
      };
      expr = "rate(node_disk_io_time_weighted_seconds_total{job=\"integrations/node_exporter\", device!=\"\"}[5m]) > 10\n";
      for = "30m";
      labels = {severity = "warning";};
    }
    {
      alert = "NodeSystemdServiceFailed";
      annotations = {
        description = "Systemd service {{ $labels.name }} has entered failed state at {{ $labels.instance }}";
        summary = "Systemd service has entered failed state.";
      };
      expr = "node_systemd_unit_state{job=\"integrations/node_exporter\", state=\"failed\"} == 1\n";
      for = "5m";
      labels = {severity = "warning";};
    }
    {
      alert = "NodeSystemdServiceFailedIsNonZero";
      annotations = {
        description = "Systemd service(s) have entered a failed state at {{ $labels.instance }} with total services failed of: {{ $value }}";
        summary = "Systemd service(s) have entered a failed state.";
      };
      expr = "node_systemd_units{job=\"integrations/node_exporter\", state=\"failed\"} > 0\n";
      for = "5m";
      labels = {severity = "warning";};
    }
    {
      alert = "NodeOomDetected";
      annotations = {
        summary = "The OOM killer has been active in the past hour.";
        description = "{{ $labels.instance }} has had {{ $value }} OOM killing(s) in the past hour. Please investigate.";
      };
      expr = ''increase(node_vmstat_oom_kill[1h]) > 0'';
      for = "5m";
      labels.severity = "page";
    }
  ];
}
